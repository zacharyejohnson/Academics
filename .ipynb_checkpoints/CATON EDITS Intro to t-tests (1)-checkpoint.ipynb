{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9d86c1",
   "metadata": {},
   "source": [
    "# Incomplete Data \n",
    "\n",
    "Creating accurate predictions is one of the most valuable skills in the job market today. Statisticians, economists, and data scientists use data gathered from specific populations in order to make predictions about what behaviors are likely to occur in the future, or what the truth is about what has already occured. Through computational and statistical techniques, we can make _statistical inferences_ to draw conclusions from data that are often incomplete.\n",
    "\n",
    "As far as estimations of parameters that already exist, having full population data would mean that our questions about that population are answered. But because the cost of gathering full population would, usually, outweigh the benefit of having perfectly accurate data, we are okay with using incomplete samples to make inferences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61913fb",
   "metadata": {},
   "source": [
    "## Are Mutual Funds better than Broad-market index funds? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a3f826",
   "metadata": {},
   "source": [
    "The term “index fund” refers to the investment approach of a fund. Specifically, it is a fund that that aims to match the performance of a particular market index, such as the S&P 500 or Russell 2,000. The index fund simply tries to match the market. This differs from a more actively managed fund, in which investments are picked by a fund manager in an attempt to beat the market. The age old question is: are the fees payed to an actively-managed mutual fund worth it? \n",
    "\n",
    "We could simply compare the mean return from a given date range for a mutual fund and compare it to the S&P500s mean return from the same time interval, and see which is higher. But the fact that mutual funds have a finite number of time intervals which we can sample means we cannnot rule out the possibility that higher or lower returns from the fund were a result of random variation, and not an indicator of the true quality of the fund. The S&P500 is , essentially, the market. We know all of the information we need about it, because it isnt a sample. The mutual fund data, on the other hand, is incomplete. So, we need to analyze the two funds using statistical techniques which account for random variation that is possible from incomplete data. \n",
    "\n",
    "We would like to ananlyze which(if any) mutual funds have out-performed the market, fees included. To start, we read the CSV downloaded from [Stock Market MBA](https://stockmarketmba.com/listoftop100activelymanagedusstockmutualfunds.php), which shows the 100 largest actively-managed mutual funds in the US. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53de08b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Top100MutualFunds.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-588f21e20c25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmutual_fund_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Top100MutualFunds.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mto_drop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"Category2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Category1\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Category3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Morningstar Rating\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Current yield\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Action\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmutual_fund_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmutual_fund_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mto_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Top100MutualFunds.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "mutual_fund_data = pd.read_csv(\"Top100MutualFunds.csv\")\n",
    "to_drop = [\"Category2\", \"Category1\",\"Category3\", \"Morningstar Rating\",\"Current yield\", \"Action\"]\n",
    "mutual_fund_data = mutual_fund_data.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf09e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_fund_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced80df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader as web\n",
    "import datetime as dt\n",
    "\n",
    "mutual_fund_dict = {}\n",
    "symbols = mutual_fund_data[\"Symbol\"][25:50]\n",
    "# will only analyze the first 25 funds for now, but this should illustrate how to do it for any fund \n",
    "start= dt.datetime(1970, 1, 1)\n",
    "end = dt.datetime.today()\n",
    "for symbol in symbols: \n",
    "    #pull mutual fund data for the longest timeframe avaliable, and cpnvert to monthly percent change data \n",
    "    fund_data = web.DataReader(symbol, 'yahoo', start, end)#[\"Adj Close\"].resample('M').first().pct_change()\n",
    "    mutual_fund_dict[symbol] = fund_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcdf147",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_fund_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546cb57",
   "metadata": {},
   "source": [
    "To start, you formulate your __hypotheses__. These are mutually exclusive, falsifiable statements. Only one can be true, and one of them will be true. We create these two hypotheses: \n",
    "\n",
    "- The _null_ hypothesis $H_o$: The true means of the the sample populations do not differ.\n",
    "- The _alternate_ hypothesis $H_a$: The true means of the sample populations do differ.\n",
    "\n",
    "### 4 Steps of Hypothesis Testing\n",
    "\n",
    "All hypotheses are tested using a four-step process:\n",
    "\n",
    "1. State the two hypotheses so that only one can be right. \n",
    "2. Formulate an analysis plan, which outlines how the data will be evaluated.\n",
    "3. Carry out the plan and physically analyze the sample data.\n",
    "4. Analyze the results and either reject the null hypothesis, or state that the null hypothesis is plausible, given the data.\n",
    "\n",
    "Hypothesis testing can be done mentally. It would be burdensome to have to state your _null_ and _alternate_ hypotheses, and run through these four steps explicitly every time you made a predictive computer model. The point is that in means testing, there is a clear process and result that deliniates \"Yes, the true means of these samples are different\" and \"No, they're not significantly different\"\n",
    "\n",
    "In the case of us determining the efficacy of our company's marketing campaign, these are our hypotheses: \n",
    "\n",
    "- $H_o$: There is no difference between the mutual fund's and S&P500's average monthly return. \n",
    "- $H_a$: The mutual funds have a higher mean gain than the S&P500. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = mutual_fund_dict.keys()\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eedcf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dictionary to hold average yearly gain for each mutual fund \n",
    "monthly_returns_dict = {}\n",
    "keys = mutual_fund_dict.keys()\n",
    "for key in keys:\n",
    "    # for each mutual fund, find average yearly gain\n",
    "    monthly_returns_dict[key] = mutual_fund_dict[key][\"Adj Close\"].resample(\"Y\").first().pct_change().mean()\n",
    "monthly_returns_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8918aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_returns_dict = {}\n",
    "for key in keys:\n",
    "    # for each mutual fund, find average yearly gain\n",
    "    yearly_returns_dict[key] = mutual_fund_dict[key][\"Adj Close\"].resample(\"Y\").first().pct_change().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52845a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_returns_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe99eee",
   "metadata": {},
   "source": [
    "These values will be compared to the monthly returns of the stock market: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datlib.stats import *\n",
    "\n",
    "sp500 = web.DataReader('^GSPC', 'yahoo', start, end)['Adj Close'].resample('Y').first().pct_change().dropna()\n",
    "mean_sp500_gain = mean(sp500)\n",
    "\n",
    "mean_sp500_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9962c2",
   "metadata": {},
   "source": [
    "##### T Distributions\n",
    "All of the t-distributions below are normal distributions. As the degrees of freedom increases past 30 or so, the distribution becomes the _standard normal distribution_(see Central Limit Theorem below), which has a standard deviation of 1 and mean of 0, and we use z-scores to analyze this. \n",
    "\n",
    "__The $t$ value tells us how many standard deviations away from the mean our sample sits on a $t$ distribution of the _differences_ of these two means, where the mean of the distribution is zero.__\n",
    "The t-distribution changes based on sample size, as increased sample size allows for higher _degrees of freedom_, which are defined for two samples as: \n",
    "\n",
    "- $df = (N_1 + N_2)  – 2$\n",
    "\n",
    "And for a single sample as: \n",
    "\n",
    "- $df = N - 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd2156",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9e8b8",
   "metadata": {},
   "source": [
    "# Comparisons of Means\n",
    "\n",
    "When dealing with a population of known parameters $\\mu$ and $\\sigma^2$, we can take any mean $\\bar{X}$ gotten from a sample and determine the likelihood that the sample came from out known population, or a population with same mean as our known population. We do this using a z-score: \n",
    "<h3 align=\"center\">\n",
    "    <font size=\"5\">\n",
    "        $ z = \\frac{\\bar{X} - \\mu}{\\sigma}$\n",
    "    </font>\n",
    "</h3>\n",
    "\n",
    "### Central Limit Theorem:\n",
    "\n",
    "\n",
    "If $\\bar{X}$ is the mean of a random sample of size $n$ taken\n",
    "from a population with mean $\\mu$ and finite variance $\\sigma^2$, then the limiting form of\n",
    "the distribution of\n",
    "<h3 align=\"center\">\n",
    "    <font size=\"5\">\n",
    "        $ z = \\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}$\n",
    "    </font>\n",
    "</h3>\n",
    "\n",
    " as $\\lim_{n \\to \\infty}$, is the *standard normal distribution* \n",
    " \n",
    " The power of the CLT is that this holds no matter the type of distribution we are sampling from. So, for instance, if we took 30 random samples from a lognormal distribution, the means of the samples would be normally distributed. \n",
    " \n",
    " The Z-value tells us: what is the probability that a given sample mean would occur given the sample size and population mean? As n gets larger, the mean is expected to get more accurate if it does follow the population mean $\\mu$\n",
    " \n",
    "The gotten _z-score_ tells us how many standard deviations our sample mean $\\bar{X}$ is from our population mean $\\mu$.\n",
    " \n",
    " The normal approximation for $\\bar{X}$ will generally be good if $n$ ≥ 30, provided the population distribution is not terribly skewed. If $n$ < 30, the approximation is good only if the population is not too different from a normal distribution and, as stated above, if the population is known to be normal, the sampling distribution of $\\bar{X}$ will follow a normal distribution exactly, no matter how small the size of the samples.\n",
    " \n",
    "For the following demonstration, [ImageMagick](https://imagemagick.org/script/download.php#windows) must be installed as well as wand through pip install. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wand\n",
    "\n",
    "# must install ImageMagick as well; but not necessary if you just want to view the plot included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f232298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central Limit Theorem\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation as animation\n",
    "from IPython import display\n",
    "%matplotlib notebook\n",
    "# number of simulations of die roll\n",
    "n = 200\n",
    "\n",
    "# In each simulation, there is one trial more than the previous simulation\n",
    "dist_avgs = {\"Die Rolls\":[],\n",
    "            \"Possoin\":[]}\n",
    "\n",
    "for i in range(n):\n",
    "    dist_avgs[\"Die Rolls\"].append(np.average(np.random.randint(1,7,n)))\n",
    "    dist_avgs[\"Possoin\"].append(np.average(np.random.poisson(1, n)))\n",
    "dist_avgs = pd.DataFrame(dist_avgs)\n",
    "dist_avgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d7dc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,8))\n",
    "dist_avgs.plot.hist(density = True, ax = ax, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b0b7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that will plot the histogram, where current is the latest figure\n",
    "def clt(current, *kwargs):\n",
    "    # if animation is at the last frame, stop it\n",
    "    if current == n:\n",
    "        a.event_source.stop()\n",
    "\n",
    "    for i in range(len(dist_avgs.keys())):\n",
    "        ax[i].clear()\n",
    "        dist = list(dist_avgs.keys())[i]\n",
    "        dist_avgs[dist].iloc[:current].plot.hist(density = True,\n",
    "                                                 color = \"C\" + str(i),\n",
    "                                   ax = ax[i])\n",
    "#         ax[i].hist(dist_avgs[dist][0:current], bins= int(current/10 + 1))\n",
    "        title = dist+'\\n# Samples = {}'\n",
    "        ax[i].set_title(title.format(current))\n",
    "        ax[i].set_xlabel(xlabel)\n",
    "        ax[i].set_ylabel(ylabel)\n",
    "#     plt.annotate('Die roll = {}'.format(current), [3,27])\n",
    "fig, ax = plt.subplots(1,2, figsize = (10,5))\n",
    "xlabel, ylabel = 'Sample Average Value', 'Density'\n",
    "kwargs = (ax, xlabel, ylabel, n, dist_avgs)\n",
    "a = animation(fig, clt, interval=20, frames=n, fargs = kwargs)\n",
    "# video = a.to_html5_video()\n",
    "# html = display.HTML(video)\n",
    "# display.display(html)\n",
    "# a.save('clt2.gif', writer='imagemagick', fps=10)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2186ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.save(\"clt2.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a85c927",
   "metadata": {},
   "source": [
    "This is the gif we just produced, embedded: \n",
    "\n",
    "<img src=\"clt2.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31a702d",
   "metadata": {},
   "source": [
    "So, for any sample with $n$ > 30, $\\bar{x}$ can be substituted for $\\mu$ and $s$ can be substituted for $\\sigma$\n",
    " \n",
    "This Z-test asunes that we have access to the population standard deviation and mean _or_ that $n$ is large enough (>30) for $s^2$ and $\\bar{x}$ to be used as a reliable estimate for $\\sigma^2$ and $\\mu$. When these conditions do not hold, and we do not have a large enough sample or sufficient population data, we need another estimator.  \n",
    " \n",
    "\n",
    "The __T-test__ is used when we are dealing with a population of unknown distribution, and would like to compare a given sample mean to one of three options: \n",
    "\n",
    "- **One Sample T-test:** The one sample t test compares the mean of your sample data to a known value. For example, you might want to know how your sample mean compares to the population mean, like our value of 120,000 for average mothly store revenue\n",
    "<h3 align=\"center\">\n",
    "    <font size=\"7\">\n",
    "        $ t = \\frac{\\bar{X} - \\mu}{\\frac{s}{\\sqrt{n}}}$\n",
    "    </font>\n",
    "    </h3> \n",
    "    \n",
    "    - Null Hypothesis: sample mean is the same as hypothesized or theoretical mean\n",
    "    - Alternative Hypothesis: sample mean is different from the hypothesized or theoretical mean\n",
    "    \n",
    "\n",
    "- **Independent Samples T-test:** The The independent samples t test (also called the unpaired samples t test) is the most common form of the T test. It helps you to compare the means of two sets of data. Normally, we are checking to see if the means of the data are significantly different from a differnece of zero. But , we can also check if they are significantly different from a hypothesized or theoretical value. For instance, say we had two groups of males and one group of females and we wanted to compare average heights between the groups. For the males, we would check to see if they differed significantly from an average height difference of zero, whereas when comparing the males to the females we may want to see if they were significantly different from an average difference of 2 inches, or whatever the average height between males and females is. **This hypothesized difference, $(\\mu_1 - \\mu_2)$, will usually be zero, but not always.**\n",
    "\n",
    "<h3 align=\"center\">\n",
    "    <font size=\"7\">\n",
    "        $ t = \\frac{(\\bar{x_1}-\\bar{x_2})-(\\mu_1 - \\mu_2)}{\\sqrt{\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2}}}$\n",
    "    </font>\n",
    "    </h3> \n",
    "    \n",
    "   \n",
    "   - Note that this t-test is one variation of the independent samples t-test that _does not_ assume equal variance between the samples\n",
    "   \n",
    "    - Null Hypothesis: sample mean is the same as hypothesized or theoretical mean\n",
    "    - Alternative Hypothesis: sample mean is different from the hypothesized or theoretical mean\n",
    "    \n",
    "    \n",
    "- **Paired Samples T-test:** A paired t test (also called a correlated pairs t-test, a paired samples t test or dependent samples t test) is where you run a t test on dependent samples. Dependent samples are essentially connected — they are tests on the same person or thing. This would be useful if we chose a random sample of stores and measured their mean revenues before and after implementation of the new marketing campaign as our two means. For our function, we can simply add an optional argument \"equal_var\" to our independent t-test funtion which will cause it to act like a paired samples t-test. \n",
    "\n",
    "The t-value we obtain will lie on the horizontal axis of our t-distribution, representing the number of standard deviations the difference between our sample mean and theoretical mean lies from zero, with a corresponding p-value on the y-axis that tells us how likely our result would be if the population our sample was drawn from had the same mean as our theorized or population mean. This t-distribution takes the form: \n",
    "<h3 align=\"center\">\n",
    "    <font size=\"6\">\n",
    "        $ f(T) = \\frac{(1 + \\frac{T^2}{\\nu})^{\\frac{-(\\nu+1)}{2}}}{B(0.5,0.5\\nu)\\sqrt(\\nu)}$\n",
    "    </font>\n",
    "    </h3> \n",
    "    \n",
    "    \n",
    "- Where $\\nu$ is the degrees of freedom of the distribution and B is the beta function, which is beyond the scope of this book and can be pulled from the scipy.stats library. \n",
    "\n",
    "### T-distribution p-value\n",
    "\n",
    "As we can see, a lower sample size, and hence a lower degrees of freedom, leads to a lower probaility that our t-score is near 0 when our population means are the same, because more random variation is likely when the sample size is so low. The point of a t-score is to determine if the difference in the two means of the samples is too drastic for the true population means to be the same. As we approacch 30 with our degrees of freedom, the graph doesnt change much, and this is a standard normal distribution, which the z-score uses. That is why we use z-score for large sample sizes. \n",
    "\n",
    "Once we get our t-score based on the t-distribution, shown on the x-axis of the above graph, we get a corresponding __p-value__, shown on the y-axis. This value is the probability of our gotten t-value if the true means were the same. \n",
    "\n",
    "- If the corresponding p-value from our t-value is too low, we choose to __reject the null hypothesis $H_o$__, and say that our samples come from different populations who's means are different. This is a \"statistically significant\" result. \n",
    "\n",
    "\n",
    "- If the p-value is sufficiently high, we __fail to reject the null hypothesis $H_o$__, and say that there is a high enough chance that the samples came from populations with the same means. This is a \"statistically insignificant\" result. \n",
    "\n",
    "\n",
    "- The value at which a non-significant result becomes a significant one is called the __*critical value*__, denoted $\\alpha$, and is most commonly 0.05.\n",
    "\n",
    "We now implement the theory into code: \n",
    "\n",
    "First, we create a funtion that uses a flat np.linspace array from -10 to 10(very extreme t vaues) and transform it according to the t-distributions density function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a9f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datlib.stats\n",
    "import scipy.special as sc\n",
    "import scipy.stats as stats\n",
    "\n",
    "# define a function to create the actual distribution from which we can analyze our t value from the t test\n",
    "def create_t_distribution(df):\n",
    "    x = np.linspace(-5, 5, 1000) # large number of points will ensure accuracy\n",
    "    # transform flat array of x values into t distribution\n",
    "    t_distribution = ((1+x**2/df)**(-(df+1)/2))/(sc.beta(.5, .5*df)*np.sqrt(df))\n",
    "    return t_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aebd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "cauchy = create_t_distribution(1)\n",
    "ax.plot(cauchy, '-', lw=3, alpha=1,  label = \"Cauchy\", color='b')\n",
    "t_df = [2, 3, 4, 5, 10, 20]\n",
    "for df in t_df:\n",
    "    dist = create_t_distribution(df)\n",
    "    ax.plot(dist, '-', lw=1, alpha=df/20,  label = \"df: \"+ str(df), color='k')\n",
    "gaussian = create_t_distribution(30)\n",
    "ax.plot(gaussian, lw=3, alpha=1, color = 'r',  label='Standard Normal Distribution')\n",
    "plt.rcParams.update({\"font.size\": 15})\n",
    "ax.set_ylabel(\"Probability of t-score\")\n",
    "ax.set_xlabel(\"Standard Deviations away from mean( this varies as the distributions have different SD's)\")\n",
    "plt.title(\"T-distribution with varying degrees of freedom\")\n",
    "ax.set_xticklabels(labels = \"\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a30ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that allows us to evaluate the t distribution at a given t value and df\n",
    "def t_prob(t, df): \n",
    "    p_value = stats.t.sf(t, df)\n",
    "    # equivalent to = ((1 + t**2 / df)**(-(df + 1) / 2)) / (sc.beta(.5, .5 * df) * np.sqrt(df))\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a1815b",
   "metadata": {},
   "source": [
    "Next, we create a function that will calculate a t value from a given sample set and $\\mu$ value and output the t value and its corresponding p-value from the distribution we created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c0aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datlib.stats import *\n",
    "def ttest_one_sample(data, mu):\n",
    "    x_bar = mean(data)\n",
    "    s = SD(data, sample=True)\n",
    "    n = len(data)\n",
    "    df = n - 1\n",
    "    t = (x_bar - mu) / (s / np.sqrt(n))\n",
    "    p_value = t_prob(t, df)\n",
    "    if p_value > .05:\n",
    "        return_string = \"T-value: \" + str(t) + \", P-value: \" + str(\n",
    "            p_value) + \", Fail to reject null hypothesis.\"\n",
    "    else:\n",
    "        return_string = \"T-value: \" + str(t) + \", P-value: \" + str(\n",
    "            round(p_value, 5)) + \", Reject null hypothesis.\"\n",
    "\n",
    "    return return_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cbbaa7",
   "metadata": {},
   "source": [
    "To show the utility of the single-sample t-test, we can check to see if the S&P500's average yearly return was significantly different from some arbitrary value, like 10%. Can we confidently say that the S&P500's return was different from 10%, or could the observed difference have been due to random variation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_one_sample(sp500, .10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2350f5ad",
   "metadata": {},
   "source": [
    "We can test our results vs the SciPy library's one sample t test: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ee8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_1samp(sp500, .10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa20b3",
   "metadata": {},
   "source": [
    "And the results are essentially the same, but the loss of significant figures many decimal places out become multiplied later on in our function to give slightly different results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffbbdc9",
   "metadata": {},
   "source": [
    "From the test, we fail to reject our null hypothesis that the average yearly S&P500 gain was not significantly different from 10%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a80a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# independent samples t-test, setting equal_var=True will turn this test into a paired samples t-test where equal variance \n",
    "# is assumed \n",
    "def ttest_ind_samp(a, b, hypothesized_difference = 0, equal_var=False): \n",
    "    s1 = variance(a)\n",
    "    s2 = variance(b)\n",
    "    n1 = len(a)\n",
    "    n2 = len(b)\n",
    "# if paired samples, function is simpler \n",
    "    if (equal_var):\n",
    "        df = n1 + n2 - 2\n",
    "        svar = ((n1 - 1) * s1 + (n2 - 1) * s2) / float(df)\n",
    "        denom = np.sqrt(svar * (1.0 / n1 + 1.0 / n2))\n",
    "    else:\n",
    "        vn1 = s1 / n1\n",
    "        vn2 = s2 / n2\n",
    "        df = ((vn1 + vn2)**2) / ((vn1**2) / (n1 - 1) + (vn2**2) / (n2 - 1))\n",
    "        denom = np.sqrt(vn1 + vn2)\n",
    "  \n",
    "    x_bar1 = np.mean(a)\n",
    "    x_bar2 = np.mean(b)\n",
    "    d = np.mean(a) - np.mean(b) - hypothesized_difference\n",
    "    t = d / denom\n",
    "    #df_approx = round(df, 0)\n",
    "    # t = ((x_bar1 - x_bar2) - (hypothesized_difference)) / (np.sqrt( (s1 / n1) + ( s2 / n2)))\n",
    "    p_value = t_prob(t, df)\n",
    "    if p_value > .05:\n",
    "        return_string = \"T-value: \" + str(t) + \", P-value: \" + str(\n",
    "            p_value) + \", Fail to reject null hypothesis.\"\n",
    "    else:\n",
    "        return_string = \"T-value: \" + str(t) + \", P-value: \" + str(\n",
    "            round(p_value, 5)) + \", Reject null hypothesis.\"\n",
    "\n",
    "    return return_string\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a597804",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for key in yearly_returns_dict.keys():\n",
    "    print(\"\\n\"+key+\": \")\n",
    "    print(stats.ttest_ind(yearly_returns_dict[key], sp500, alternative=\"less\"))\n",
    "    i+=1\n",
    "    if i == 7: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af7b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for key in yearly_returns_dict.keys():\n",
    "    print(\"\\n\"+key+\": \")\n",
    "    print(ttest_ind_samp(yearly_returns_dict[key], sp500))\n",
    "    i+=1\n",
    "    if i == 7: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2b3e65",
   "metadata": {},
   "source": [
    "The results are not exactly the same but they are reasonably close "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f36cea4",
   "metadata": {},
   "source": [
    "# ANOVA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90589d09",
   "metadata": {},
   "source": [
    "While using T-tests and Z-tests to analyze means of groups, we were restricted to only being able to compare two groups at a time. What if we wanted to see of there was significant differences between more than two groups? The **ANOVA**, or **Analysis of Variance** techniques allow us to test the null hypothesis that there is no significant difference between $k$ (some integer larger than 2) groups. \n",
    "\n",
    "- $H_o$: $\\mu_1 = \\mu_2 = \\cdots = \\mu_k$\n",
    "- $H_a$: At least two of the means are not equal. \n",
    "\n",
    "### Assumptions needed for ANOVA\n",
    "There are three assumptions that must be met in order to carry out an ANOVA test: \n",
    "\n",
    "1. The experimental errors of yoyr data are normally distributed\n",
    "2. Homoscedasticity - the variances of your factors are all roughly the same (and at least follow the same distribution)\n",
    "3. Samples are independent - Selection of one sample had no effect on any other sample\n",
    "\n",
    "### F-Statistic\n",
    "The distribution used for the hypothesis test is a new one. It is called the F distribution, named after Sir Ronald Fisher, an English statistician. The F-statistic is a ratio. There are two sets of degrees of freedom; one for the numerator and one for the denominator. \n",
    "\n",
    "The F distribution is derived from the t-distribution. The values of the F distribution are squares of the\n",
    "corresponding values of the t-distribution. One-Way ANOVA expands the t-test for comparing more than two groups.\n",
    "The scope of that derivation is beyond the level of this textbook. \n",
    "\n",
    "To calculate the F ratio, two estimates of the variance are made:\n",
    "\n",
    "1. **Variance between samples**: An estimate of $\\sigma^2$ that is the variance of the sample means multiplied by n (when the sample sizes are the same.). If the samples are different sizes, the variance between samples is weighted to account for the different sample sizes. The variance is also called **variation due to treatment or explained variation.**\n",
    "\n",
    "2. **Variance within samples**: An estimate of $\\sigma^2$ that is the average of the sample variances (also known as a pooled variance). When the sample sizes are different, the variance within samples is weighted. The variance is also called **the variation due to error or unexplained variation.**\n",
    "\n",
    "- $SS_b$ = the sum of squares that represents the variation among the different samples\n",
    "\n",
    "- $SS_w$ = the sum of squares that represents the variation within samples that is due to chance.\n",
    "\n",
    "To find a \"sum of squares\" means to add together squared quantities that, in some cases, may be weighted. We used sum of squares to calculate the sample variance and the sample standard deviation. \n",
    "\n",
    "MS means \"mean square.\" $MS_b$ is the variance between groups, and $MS_w$ is the variance within groups.\n",
    "\n",
    "### Caluculating the F-Statistic\n",
    "\n",
    "- $k$ = the number of different groups\n",
    "- $n_j$ = the size of the $j^{th}$ group\n",
    "- $s_j$ = the sum of the values in the $j^{th}$ group\n",
    "- $n$ = total number of all the values combined (total sample size: $\\sum{n_j}$)\n",
    "- $x$ = one value: $\\sum{x}$\n",
    "- Between group variability = $SS_{total} = \\sum{x^2} - \\frac{\\sum{x^2}}{n}$\n",
    "- Explained variation: sum of squares representing variation among the different samples: $SS_{b} = \\sum{\\frac{(s_j)^2}{n_j}} - \\frac{(\\sum{(s_j)^2}}{n}$\n",
    "- Unexplained variation: sum of squares representing variation within samples due to chance:\n",
    "$SS_w = SS_{total} – SS_b$\n",
    "- $df$'s for the numerator(between samples): $df_b = k – 1$\n",
    "- $df$'s for the denominator($df$'s within samples): $df_w = k – 1$\n",
    "- Mean square (variance estimate) explained by the different groups:\n",
    "$MS_b = \\frac{SS_b}{df_b} = \\frac{SS_b}{k-1}$\n",
    "- Mean square (variance estimate) that is due to chance (unexplained): $MS_w = \\frac{SS_w}{df_w} = \\frac{SS_w}{n - k}$\n",
    "\n",
    "The one-way ANOVA test depends on the fact that $MS_b$ can be influenced by population differences among means of the several groups. Since $MS_w$ compares values of each group to its own group mean, the fact that group means might\n",
    "be different does not affect $MS_w$. The null hypothesis says that all groups are samples from populations having the same normal distribution. The alternate\n",
    "hypothesis says that at least two of the sample groups come from populations with different normal distributions. If the null hypothesis is true, $MS_b$ and $MS_w$ should both estimate the same value. \n",
    "\n",
    "Finally, we arrive at the **F-Statistic**, which will function for us as the T-Statistic did earlier this chapter, as an input into its density function to recieve a p-value telling us the likelihood of its occurence if our null hypothesis was true. \n",
    "\n",
    "- $ F = \\frac{MS_b}{MS_w}$\n",
    "\n",
    "With a density function:\n",
    "<h3 align=\"center\">\n",
    "    <font size=\"5\">\n",
    "        $ f(x, df_1, df_2) = \\frac{df_2^{df_2/2} df_1^{df_1/2} x^{df_1 / 2-1}}\n",
    "                        {(df_2+df_1 x)^{(df_1+df_2)/2}\n",
    "                         \\beta(df_1/2, df_2/2)}$\n",
    "    </font>\n",
    "    </h3> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b72b2d",
   "metadata": {},
   "source": [
    "\n",
    "where $df_1$ and $df_2$ are the\n",
    "shape parameters and\n",
    "$\\beta$ is the beta function.  The formula for the beta function\n",
    "is\n",
    "<ul>\n",
    "$B(a, b) = \\int_0^1 t^{a-1}(1-t)^{b-1}dt\n",
    "        = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$,\n",
    " \n",
    "<p>where <span class=\"math notranslate nohighlight\">\\(\\Gamma\\)</span> is the gamma function.</p>\n",
    "    \n",
    "These funtions could be implememnted manually using basic math symbols, but for our purposes, importing them from Scipy will be much more pragmatic. \n",
    "    \n",
    "In a testing context, the F distribution is treated as  \"standardized distribution\" (i.e., no location or scale parameters).\n",
    "However, in a distributional modeling context (as with other probability distributions), the F distribution itself can be\n",
    "transformed with a <a href=\"eda364.htm\">location parameter</a>, $\\mu$, and a <a href=\"eda364.htm\">scale parameter</a>, $\\sigma$.\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93309540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f as f_dist\n",
    "# create function to quickly sum squares within ANOVA test\n",
    "def sum_squares(a):\n",
    "    s = 0\n",
    "    for i in range(len(a)): \n",
    "        s += a[i]**2\n",
    "    return s\n",
    "\n",
    "# and one to square sums\n",
    "def square_sums(a):\n",
    "    s = 0\n",
    "    for i in range(len(a)): \n",
    "        s += a[i]\n",
    "    return s * s\n",
    "\n",
    "# finds value of f-distribution for given f and df1, df2\n",
    "def f_prob(f, df1, df2):\n",
    "    # use scipy to plug f-value into f distribution to return p-value\n",
    "    p_value = f_dist.sf(df1, df2, f)\n",
    "    # could attempt to manually implement, i.e\n",
    "    # [f(x, df_1, df_2) = (df_2^{df_2/2} df_1^{df_1/2} x^{df_1 / 2-1} / \n",
    "    #                   {(df_2+df_1 x)^{(df_1+df_2)/2}*sc.beta(df_1/2, df_2/2)}\\]\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use *args command to accept variable number of arguments\n",
    "def ANOVA(*args):\n",
    "    # Create numpy array of arguments using generator function\n",
    "    args = [np.asarray(arg) for arg in args]\n",
    "    # ANOVA on N groups, each in its own array\n",
    "    k = len(args)\n",
    "    # use print statements throughout funtion to ensure proper logic\n",
    "    print(\"k: \", k)\n",
    "    alldata = np.concatenate(args)\n",
    "    bign = 0\n",
    "    for i in range(k):\n",
    "        bign += len(args[i])\n",
    "    \n",
    "    print(\"n: \", bign)\n",
    "\n",
    "    # do the (x_i - x_mean) calculation that happens for every calculation of variance. This is simpler\n",
    "    # than doing the same calculation for each variable\n",
    "    grand_mean = alldata.mean()\n",
    "    alldata -= grand_mean\n",
    "    \n",
    "    \n",
    "    # \n",
    "    sstot = sum_squares(alldata) - (square_sums(alldata) / float(bign))\n",
    "    print(\"sstot: \", sstot)\n",
    "    \n",
    "    \n",
    "    ssbn = 0\n",
    "    for a in args:\n",
    "        ssbn += square_sums(a - grand_mean) / float(len(a))\n",
    "    ssbn -= (square_sums(alldata) / float(bign))\n",
    "    print(\"ssbn: \",ssbn)\n",
    "\n",
    "    sswn = sstot - ssbn\n",
    "    print(\"sswn: \", sswn)\n",
    "    \n",
    "    dfbn = k - 1\n",
    "    dfwn = bign - k\n",
    "    print(\"k: \", k, \"n: \", bign)\n",
    "    print(\"DFbn, DFwn: \", dfbn,\",\", dfwn)\n",
    "    msb = ssbn / float(dfbn)\n",
    "    print(\"msb: \", msb)\n",
    "    msw = sswn / float(dfwn)\n",
    "    print(\"msw: \", msw)\n",
    "    f = msb / msw\n",
    "\n",
    "   \n",
    "    print(\"sswn: \", sswn)\n",
    "    p_value = float(f_prob(f, dfbn, dfwn))\n",
    "\n",
    "    if p_value > .05:\n",
    "        return_string = \"F-value: \" + str(f) + \", P-value: \" + str(\n",
    "            p_value) + \", Fail to reject null hypothesis.\"\n",
    "    else:\n",
    "        return_string = \"F-value: \" + str(f) + \", P-value: \" + str(\n",
    "            round(p_value, 5)) + \", Reject null hypothesis.\"\n",
    "\n",
    "    return return_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad76c5b",
   "metadata": {},
   "source": [
    "Now that we have built our function, we can test it. In the case of our mutual fund analysis, a relevant ANOVA problem would be comparing mean returns across the categorical variable of fund category. In one of our data columns, \"Morningstar Category\", Morningstar provides categorizations of funds based of what they focus on. Does one specialization do better than the others? Should investors choose one category of fund over the others? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdc3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find what categories are listed in our dataset: \n",
    "fund_types = mutual_fund_data[\"Morningstar Category\"].unique()\n",
    "fund_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists of categories to be analyzed and pull mean yeary returns for each category\n",
    "fund_type_returns_dict = {}\n",
    "for ftype in fund_types: \n",
    "    fund_type_returns_dict[ftype] = []\n",
    "    for i in range(25, 50): \n",
    "        if mutual_fund_data[\"Morningstar Category\"][i] == ftype:\n",
    "            fund_type_returns_dict[ftype].extend(yearly_returns_dict[mutual_fund_data[\"Symbol\"][i]])\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398d44d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_fund_data[\"Morningstar Category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c02c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_type_returns_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a0c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_type_returns_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4895cd0",
   "metadata": {},
   "source": [
    "There are almost no funds with the last 5 categories so we will just do the analysis on the fist 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b32b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANOVA(fund_type_returns_dict[\"Large Growth\"],\n",
    "      fund_type_returns_dict['Large Value'],\n",
    "      fund_type_returns_dict['Large Blend'],\n",
    "      fund_type_returns_dict['Mid-Cap Growth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9b892b",
   "metadata": {},
   "source": [
    "Compare to equivalent SciPy function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad1c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.f_oneway(fund_type_returns_dict[\"Large Growth\"],\n",
    "      fund_type_returns_dict['Large Value'],\n",
    "      fund_type_returns_dict['Large Blend'],\n",
    "      fund_type_returns_dict['Mid-Cap Growth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f87372",
   "metadata": {},
   "source": [
    "These results suggest that almost all of the total variance in the data is caused by within-data variance, not between-data variance. The practical interperetation is that the type of mutual fund does not significantly change yearly returns. This result may support the EMH because specific specialization of funds has no effect on returns. The market gave the same returns to the funds no matter their specialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a0292a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa528a10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
